{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyO9YnnB5XzpZ4WdqbNIFkkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvrjsharma/HugginFace_Gradio/blob/main/DDColor_Gradio_Demo_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/piddnad/DDColor.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svmR1KiCyrFC",
        "outputId": "28a723aa-f024-4cf9-8f74-ac537b8405ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DDColor'...\n",
            "remote: Enumerating objects: 189, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 189 (delta 54), reused 156 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (189/189), 13.32 MiB | 33.60 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DDColor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxsER1SRzKRF",
        "outputId": "5cb5e427-83de-4a1f-de4e-d10126a97547"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DDColor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DXDJQk1yj5V",
        "outputId": "b226c765-3ca8-401a-b090-8c31bc881838"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dlib==19.24.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (19.24.2)\n",
            "Collecting lmdb==1.4.1 (from -r requirements.txt (line 2))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.24.3 (from -r requirements.txt (line 3))\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv_python==4.7.0.72 (from -r requirements.txt (line 4))\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==10.1.0 (from -r requirements.txt (line 5))\n",
            "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (6.0.1)\n",
            "Requirement already satisfied: Requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.31.0)\n",
            "Collecting scipy==1.9.1 (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.9.2 (from -r requirements.txt (line 9))\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.7.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.7.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTA1TwvFk5KN",
        "outputId": "3a04445f-357a-405d-9142-773c4ed23610"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.1/305.1 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm gradio_imageslider -q"
      ],
      "metadata": {
        "id": "3q2P7gxy628B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL98iRYByFjA",
        "outputId": "14e4daef-2c2c-4919-bf7a-5c214126ef41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.0/443.0 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install modelscope -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modelscope.hub.snapshot_download import snapshot_download\n",
        "\n",
        "model_dir = snapshot_download('damo/cv_ddcolor_image-colorization', cache_dir='./modelscope')\n",
        "print('model assets saved to %s'%model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGEePNw3ye84",
        "outputId": "e993f1e9-5975-4e66-f026-d9ec86e58b49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-19 03:47:46,140 - modelscope - INFO - PyTorch version 2.1.0+cu121 Found.\n",
            "2024-01-19 03:47:46,144 - modelscope - INFO - TensorFlow version 2.15.0 Found.\n",
            "2024-01-19 03:47:46,145 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-01-19 03:47:46,147 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
            "2024-01-19 03:47:46,210 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 d1bf70dc840bbc78c6b639e013c9762e and a total number of 953 components indexed\n",
            "2024-01-19 03:47:50,058 - modelscope - WARNING - Model revision not specified, use revision: v1.02\n",
            "Downloading: 100%|██████████| 1.39k/1.39k [00:00<00:00, 6.37MB/s]\n",
            "Downloading: 100%|██████████| 235k/235k [00:00<00:00, 7.11MB/s]\n",
            "Downloading: 100%|██████████| 199k/199k [00:00<00:00, 6.00MB/s]\n",
            "Downloading: 100%|██████████| 94.9k/94.9k [00:00<00:00, 5.95MB/s]\n",
            "Downloading: 100%|██████████| 117k/117k [00:00<00:00, 4.92MB/s]\n",
            "Downloading: 100%|█████████▉| 870M/870M [00:13<00:00, 67.0MB/s]\n",
            "Downloading: 100%|██████████| 3.44k/3.44k [00:00<00:00, 13.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model assets saved to ./modelscope/damo/cv_ddcolor_image-colorization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Would go at the very top of app.py for gradio demo\n",
        "import sys\n",
        "sys.path.append('/content/DDColor')\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from basicsr.archs.ddcolor_arch import DDColor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gradio as gr\n",
        "from gradio_imageslider import ImageSlider\n",
        "import uuid\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "e3TU4y5G7QQJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageColorizationPipeline(object):\n",
        "\n",
        "    def __init__(self, model_path, input_size=256, model_size='large'):\n",
        "\n",
        "        self.input_size = input_size\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "        if model_size == 'tiny':\n",
        "            self.encoder_name = 'convnext-t'\n",
        "        else:\n",
        "            self.encoder_name = 'convnext-l'\n",
        "\n",
        "        self.decoder_type = \"MultiScaleColorDecoder\"\n",
        "\n",
        "        if self.decoder_type == 'MultiScaleColorDecoder':\n",
        "            self.model = DDColor(\n",
        "                encoder_name=self.encoder_name,\n",
        "                decoder_name='MultiScaleColorDecoder',\n",
        "                input_size=[self.input_size, self.input_size],\n",
        "                num_output_channels=2,\n",
        "                last_norm='Spectral',\n",
        "                do_normalize=False,\n",
        "                num_queries=100,\n",
        "                num_scales=3,\n",
        "                dec_layers=9,\n",
        "            ).to(self.device)\n",
        "        else:\n",
        "            self.model = DDColor(\n",
        "                encoder_name=self.encoder_name,\n",
        "                decoder_name='SingleColorDecoder',\n",
        "                input_size=[self.input_size, self.input_size],\n",
        "                num_output_channels=2,\n",
        "                last_norm='Spectral',\n",
        "                do_normalize=False,\n",
        "                num_queries=256,\n",
        "            ).to(self.device)\n",
        "\n",
        "        self.model.load_state_dict(\n",
        "            torch.load(model_path, map_location=torch.device('cpu'))['params'],\n",
        "            strict=False)\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def process(self, img):\n",
        "        self.height, self.width = img.shape[:2]\n",
        "        img = (img / 255.0).astype(np.float32)\n",
        "        orig_l = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)[:, :, :1]  # (h, w, 1)\n",
        "\n",
        "        # resize rgb image -> lab -> get grey -> rgb\n",
        "        img = cv2.resize(img, (self.input_size, self.input_size))\n",
        "        img_l = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)[:, :, :1]\n",
        "        img_gray_lab = np.concatenate((img_l, np.zeros_like(img_l), np.zeros_like(img_l)), axis=-1)\n",
        "        img_gray_rgb = cv2.cvtColor(img_gray_lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "        tensor_gray_rgb = torch.from_numpy(img_gray_rgb.transpose((2, 0, 1))).float().unsqueeze(0).to(self.device)\n",
        "        output_ab = self.model(tensor_gray_rgb).cpu()  # (1, 2, self.height, self.width)\n",
        "\n",
        "        # resize ab -> concat original l -> rgb\n",
        "        output_ab_resize = F.interpolate(output_ab, size=(self.height, self.width))[0].float().numpy().transpose(1, 2, 0)\n",
        "        output_lab = np.concatenate((orig_l, output_ab_resize), axis=-1)\n",
        "        output_bgr = cv2.cvtColor(output_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        output_img = (output_bgr * 255.0).round().astype(np.uint8)\n",
        "\n",
        "        return output_img\n"
      ],
      "metadata": {
        "id": "7OdWn35O7Li1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'modelscope/damo/cv_ddcolor_image-colorization/pytorch_model.pt'\n",
        "input_size = 512\n",
        "model_size = 'large'\n",
        "colorizer = ImageColorizationPipeline(model_path=model_path,\n",
        "                                      input_size=input_size,\n",
        "                                      model_size=model_size)\n",
        "\n",
        "def colorize(img):\n",
        "  image_out = colorizer.process(img)\n",
        "  # Generate a unique filename using UUID\n",
        "  unique_imgfilename = str(uuid.uuid4()) + '.png'\n",
        "  cv2.imwrite(unique_imgfilename, image_out)\n",
        "  return (img, unique_imgfilename)\n"
      ],
      "metadata": {
        "id": "_tuKrf7J9_J5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      bw_image = gr.Image(label='Black and White Input Image')\n",
        "      btn = gr.Button('Convert using DDColor')\n",
        "    with gr.Column():\n",
        "      col_image_slider = ImageSlider(position=0.5,\n",
        "                                     label='Colored Image with Slider-view')\n",
        "\n",
        "  btn.click(colorize, bw_image, col_image_slider)\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "nljzj0K73LJP",
        "outputId": "2710657a-4116-4a24-a23e-61ed172f642e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://3470f1249c2a84dd0c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3470f1249c2a84dd0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "1f71309b-11b7-49d1-ba98-ffbcb09b175a.png\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3470f1249c2a84dd0c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def main():\n",
        "#parser = argparse.ArgumentParser()\n",
        "#model_path = 'pretrain/net_g_200000.pth'\n",
        "model_path = 'modelscope/damo/cv_ddcolor_image-colorization/pytorch_model.pt'\n",
        "input_size = 512\n",
        "model_size = 'large'\n",
        "input = './assets/test_images'\n",
        "output =  './colorize_output'\n",
        "#parser.add_argument('--model_path', type=str, default='pretrain/net_g_200000.pth')\n",
        "#parser.add_argument('--input', type=str, default='figure/', help='input test image folder or video path')\n",
        "#parser.add_argument('--output', type=str, default='results', help='output folder or video path')\n",
        "#parser.add_argument('--input_size', type=int, default=512, help='input size for model')\n",
        "#parser.add_argument('--model_size', type=str, default='large', help='ddcolor model size')\n",
        "#args = parser.parse_args()\n",
        "\n",
        "#print(f'Output path: {args.output}')\n",
        "#os.makedirs(args.output, exist_ok=True)\n",
        "img_list = os.listdir(input)\n",
        "#assert len(img_list) > 0\n",
        "\n",
        "colorizer = ImageColorizationPipeline(model_path=model_path, input_size=input_size, model_size=model_size)\n",
        "\n",
        "for name in tqdm(img_list):\n",
        "    img = cv2.imread(os.path.join(input, name))\n",
        "    image_out = colorizer.process(img)\n",
        "    cv2.imwrite(os.path.join(output, name), image_out)\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s09UITYy70_k",
        "outputId": "53088c81-9779-4133-9b66-289b5d754355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  6.75it/s]\n"
          ]
        }
      ]
    }
  ]
}